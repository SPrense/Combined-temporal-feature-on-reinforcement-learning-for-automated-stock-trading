{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfv52r2G33jY"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL/blob/master/Stock_NeurIPS2018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading\n",
    "\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Task Description](#0)\n",
    "* [2. Install Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. A List of Python Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download and Preprocess Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5. Build Market Environment in OpenAI Gym-style](#4)  \n",
    "    * [5.1. Data Split](#4.1)  \n",
    "    * [5.3. Environment for Training](#4.2)    \n",
    "* [6. Train DRL Agents](#5)\n",
    "* [7. Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Task Discription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "We train a DRL agent for stock trading. This task is modeled as a Markov Decision Process (MDP), and the objective function is maximizing (expected) cumulative return.\n",
    "\n",
    "We specify the state-action-reward as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes many features and learns by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 consituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period.\n",
    "\n",
    "\n",
    "The data for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Install Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Ta-Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Ta-Lib -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/quantopian/pyfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "02a8a804-d120-4388-a167-20a81cb33d87"
   },
   "outputs": [],
   "source": [
    "## install finrl library\n",
    "!pip install wrds -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install swig -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. A list of Python packages \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import datetime\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa.stattools as ts\n",
    "# matplotlib.use('Agg')\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading_adjust import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models_add import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../mha-gru-drl\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RtUc_ofKmpdy"
   },
   "outputs": [],
   "source": [
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance provides stock data, financial news, financial reports, etc. Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** in FinRL-Meta to fetch data via Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "h3XJnvrbLp-C",
    "outputId": "3c4dda81-f617-4e9b-f88c-edb502d1500c"
   },
   "outputs": [],
   "source": [
    "# from config.py, TRAIN_START_DATE is a string\n",
    "TRAIN_START_DATE\n",
    "# from config.py, TRAIN_END_DATE is a string\n",
    "TRAIN_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FUnY8WEfLq3C"
   },
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2017-03-02'\n",
    "TRAIN_END_DATE = '2020-02-27'\n",
    "TRADE_START_DATE = '2020-02-28'\n",
    "TRADE_END_DATE = '2022-12-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "ts.set_token('df3c6a8e252ff736ca08a3364022e4340b68485bf9f9d3cea4c94f21')\n",
    "pro = ts.pro_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择上证50指数的成分股\n",
    "df_index = pro.index_weight(index_code='000016.sh', start_date='20121010', end_date='20220228')\n",
    "# df_index.to_csv('SSE_50_index_weight.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['601318.SH' '600016.SH' '601166.SH' '600000.SH' '600036.SH' '601328.SH'\n",
      " '601288.SH' '600030.SH' '600519.SH' '600837.SH' '601169.SH' '601398.SH'\n",
      " '601766.SH' '600887.SH' '601668.SH' '601601.SH' '601988.SH' '600104.SH'\n",
      " '600048.SH' '601818.SH' '601989.SH' '600015.SH' '600028.SH' '600637.SH'\n",
      " '601688.SH' '601390.SH' '600999.SH' '600518.SH' '601006.SH' '601857.SH'\n",
      " '600050.SH' '601186.SH' '601628.SH' '601985.SH' '600795.SH' '600585.SH'\n",
      " '600893.SH' '601088.SH' '600010.SH' '601901.SH' '601211.SH' '601669.SH'\n",
      " '600111.SH' '601336.SH' '600109.SH' '600958.SH' '601998.SH' '601800.SH'\n",
      " '600018.SH' '600150.SH'] 50\n"
     ]
    }
   ],
   "source": [
    "#选择某个时间点的上证50指数成分股作为参考跟踪股票池\n",
    "select_date = '20160229' #自定义的时间点为每个月的月末\n",
    "df_select = df_index[df_index['trade_date']==select_date]\n",
    "sort_SSEindex = df_select['con_code'].unique()\n",
    "print(sort_SSEindex,len(sort_SSEindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['601800.SH', '600795.SH', '601998.SH', '600519.SH', '601988.SH', '601688.SH', '601989.SH', '600837.SH', '601336.SH', '600048.SH', '601901.SH', '600015.SH', '600999.SH', '601169.SH', '601668.SH', '601398.SH', '600111.SH', '600036.SH', '601628.SH', '600585.SH', '600000.SH', '600887.SH', '601669.SH', '600016.SH', '600104.SH', '601186.SH', '601328.SH', '601211.SH', '600518.SH', '601766.SH', '601390.SH', '600028.SH', '601818.SH', '601288.SH', '601601.SH', '601006.SH', '601985.SH', '601318.SH', '600893.SH', '601166.SH', '600030.SH', '600018.SH', '601857.SH', '601088.SH', '600150.SH', '600958.SH', '600637.SH', '600010.SH', '600050.SH', '600109.SH']\n"
     ]
    }
   ],
   "source": [
    "#随机选择成分股中的k只股票\n",
    "import random \n",
    "k = 50\n",
    "selected_tics = random.sample(list(sort_SSEindex),k)\n",
    "print(selected_tics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>pre_close</th>\n",
       "      <th>change</th>\n",
       "      <th>pct_chg</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20170103</td>\n",
       "      <td>2307.8917</td>\n",
       "      <td>2285.2729</td>\n",
       "      <td>2311.2926</td>\n",
       "      <td>2285.2729</td>\n",
       "      <td>2286.8984</td>\n",
       "      <td>20.9933</td>\n",
       "      <td>0.9180</td>\n",
       "      <td>25908920.0</td>\n",
       "      <td>2.544074e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20170104</td>\n",
       "      <td>2322.2059</td>\n",
       "      <td>2305.9117</td>\n",
       "      <td>2324.5577</td>\n",
       "      <td>2304.1733</td>\n",
       "      <td>2307.8917</td>\n",
       "      <td>14.3142</td>\n",
       "      <td>0.6202</td>\n",
       "      <td>26104403.0</td>\n",
       "      <td>2.718326e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20170105</td>\n",
       "      <td>2322.6843</td>\n",
       "      <td>2322.4239</td>\n",
       "      <td>2326.1847</td>\n",
       "      <td>2317.8853</td>\n",
       "      <td>2322.2059</td>\n",
       "      <td>0.4784</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>26528712.0</td>\n",
       "      <td>2.656642e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20170106</td>\n",
       "      <td>2308.9363</td>\n",
       "      <td>2323.0064</td>\n",
       "      <td>2326.4342</td>\n",
       "      <td>2308.3694</td>\n",
       "      <td>2322.6843</td>\n",
       "      <td>-13.7480</td>\n",
       "      <td>-0.5919</td>\n",
       "      <td>26445124.0</td>\n",
       "      <td>2.728669e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20170109</td>\n",
       "      <td>2318.3365</td>\n",
       "      <td>2308.4423</td>\n",
       "      <td>2321.6920</td>\n",
       "      <td>2307.1634</td>\n",
       "      <td>2308.9363</td>\n",
       "      <td>9.4002</td>\n",
       "      <td>0.4071</td>\n",
       "      <td>26523074.0</td>\n",
       "      <td>2.590351e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20221226</td>\n",
       "      <td>2607.5229</td>\n",
       "      <td>2617.1819</td>\n",
       "      <td>2625.8838</td>\n",
       "      <td>2604.8991</td>\n",
       "      <td>2615.8794</td>\n",
       "      <td>-8.3565</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>19812417.0</td>\n",
       "      <td>4.156645e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20221227</td>\n",
       "      <td>2632.4526</td>\n",
       "      <td>2623.1749</td>\n",
       "      <td>2640.7395</td>\n",
       "      <td>2616.9293</td>\n",
       "      <td>2607.5229</td>\n",
       "      <td>24.9297</td>\n",
       "      <td>0.9561</td>\n",
       "      <td>21814661.0</td>\n",
       "      <td>3.985271e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20221228</td>\n",
       "      <td>2638.3055</td>\n",
       "      <td>2629.9846</td>\n",
       "      <td>2646.7069</td>\n",
       "      <td>2620.1185</td>\n",
       "      <td>2632.4526</td>\n",
       "      <td>5.8529</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>23171189.0</td>\n",
       "      <td>4.165666e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20221229</td>\n",
       "      <td>2622.0716</td>\n",
       "      <td>2622.1933</td>\n",
       "      <td>2627.4310</td>\n",
       "      <td>2605.0487</td>\n",
       "      <td>2638.3055</td>\n",
       "      <td>-16.2339</td>\n",
       "      <td>-0.6153</td>\n",
       "      <td>22776667.0</td>\n",
       "      <td>4.125150e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>000016.SH</td>\n",
       "      <td>20221230</td>\n",
       "      <td>2635.2484</td>\n",
       "      <td>2633.1636</td>\n",
       "      <td>2648.4051</td>\n",
       "      <td>2631.8491</td>\n",
       "      <td>2622.0716</td>\n",
       "      <td>13.1768</td>\n",
       "      <td>0.5025</td>\n",
       "      <td>23889757.0</td>\n",
       "      <td>4.183828e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ts_code trade_date      close       open       high        low  \\\n",
       "0     000016.SH   20170103  2307.8917  2285.2729  2311.2926  2285.2729   \n",
       "1     000016.SH   20170104  2322.2059  2305.9117  2324.5577  2304.1733   \n",
       "2     000016.SH   20170105  2322.6843  2322.4239  2326.1847  2317.8853   \n",
       "3     000016.SH   20170106  2308.9363  2323.0064  2326.4342  2308.3694   \n",
       "4     000016.SH   20170109  2318.3365  2308.4423  2321.6920  2307.1634   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1454  000016.SH   20221226  2607.5229  2617.1819  2625.8838  2604.8991   \n",
       "1455  000016.SH   20221227  2632.4526  2623.1749  2640.7395  2616.9293   \n",
       "1456  000016.SH   20221228  2638.3055  2629.9846  2646.7069  2620.1185   \n",
       "1457  000016.SH   20221229  2622.0716  2622.1933  2627.4310  2605.0487   \n",
       "1458  000016.SH   20221230  2635.2484  2633.1636  2648.4051  2631.8491   \n",
       "\n",
       "      pre_close   change  pct_chg         vol        amount  \n",
       "0     2286.8984  20.9933   0.9180  25908920.0  2.544074e+07  \n",
       "1     2307.8917  14.3142   0.6202  26104403.0  2.718326e+07  \n",
       "2     2322.2059   0.4784   0.0206  26528712.0  2.656642e+07  \n",
       "3     2322.6843 -13.7480  -0.5919  26445124.0  2.728669e+07  \n",
       "4     2308.9363   9.4002   0.4071  26523074.0  2.590351e+07  \n",
       "...         ...      ...      ...         ...           ...  \n",
       "1454  2615.8794  -8.3565  -0.3195  19812417.0  4.156645e+07  \n",
       "1455  2607.5229  24.9297   0.9561  21814661.0  3.985271e+07  \n",
       "1456  2632.4526   5.8529   0.2223  23171189.0  4.165666e+07  \n",
       "1457  2638.3055 -16.2339  -0.6153  22776667.0  4.125150e+07  \n",
       "1458  2622.0716  13.1768   0.5025  23889757.0  4.183828e+07  \n",
       "\n",
       "[1459 rows x 11 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sz = pro.index_daily(ts_code = '000016.SH',start_date='2017-04-01',end_date='2023-12-01')\n",
    "df_sz = df_sz.sort_values(['ts_code','trade_date'],ascending=True,ignore_index=True)\n",
    "df_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_sz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mema\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "df_sz.loc[2,'ema'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.000000\n",
       "1          0.000000\n",
       "2          0.000000\n",
       "3          0.000000\n",
       "4          0.000000\n",
       "           ...     \n",
       "1454    2611.166438\n",
       "1455    2612.539738\n",
       "1456    2614.202046\n",
       "1457    2614.709759\n",
       "1458    2616.034832\n",
       "Name: ema, Length: 1459, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sz.ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib as ta\n",
    "df_sz['ema'] = ta.EMA(df_sz[\"close\"], timeperiod=30)\n",
    "df_sz = df_sz.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sz = df_sz[-1411:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sz.to_csv('1230_sz.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-12-01'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRADE_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_tics = ['600104.SH','600050.SH','600048.SH','600036.SH','600031.SH','600030.SH','600028.SH','600016.SH','600009.SH','600000.SH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ts_code trade_date   open   high    low  close  pre_close  change  \\\n",
      "0     601800.SH   20221230   8.01   8.09   7.92   8.05       7.98    0.07   \n",
      "1     601800.SH   20221229   8.13   8.16   7.91   7.98       8.11   -0.13   \n",
      "2     601800.SH   20221228   8.18   8.29   8.08   8.11       8.24   -0.13   \n",
      "3     601800.SH   20221227   8.22   8.35   8.13   8.24       8.19    0.05   \n",
      "4     601800.SH   20221226   8.16   8.23   8.02   8.19       8.19    0.00   \n",
      "...         ...        ...    ...    ...    ...    ...        ...     ...   \n",
      "1444  600109.SH   20170109  13.14  13.24  13.09  13.20      13.18    0.02   \n",
      "1445  600109.SH   20170106  13.36  13.38  13.16  13.18      13.37   -0.19   \n",
      "1446  600109.SH   20170105  13.37  13.42  13.30  13.37      13.39   -0.02   \n",
      "1447  600109.SH   20170104  13.28  13.42  13.23  13.39      13.32    0.07   \n",
      "1448  600109.SH   20170103  13.05  13.51  13.02  13.32      13.03    0.29   \n",
      "\n",
      "      pct_chg        vol      amount  \n",
      "0      0.8772  448578.31  359841.007  \n",
      "1     -1.6030  643884.33  514491.306  \n",
      "2     -1.5777  571778.13  466341.341  \n",
      "3      0.6105  734200.28  605229.149  \n",
      "4      0.0000  446992.35  363487.010  \n",
      "...       ...        ...         ...  \n",
      "1444   0.1500  173705.46  228612.292  \n",
      "1445  -1.4200  163145.93  215818.139  \n",
      "1446  -0.1500  208143.43  278355.114  \n",
      "1447   0.5300  199780.20  266441.968  \n",
      "1448   2.2300  472771.97  630170.673  \n",
      "\n",
      "[72281 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "#Download随机选择的k只股票数据\n",
    "df_ts =pd.DataFrame()\n",
    "for c in selected_tics:\n",
    "    temp=pro.daily(ts_code=c,start_date=TRAIN_START_DATE,end_date=TRADE_END_DATE)\n",
    "    df_ts=pd.concat([df_ts,temp])\n",
    "print(df_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.read_csv('10_test_628.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts.to_csv('10_test_628.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(df_ts['ts_code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_date_range(df,selected_tics,k):\n",
    "    longth = []\n",
    "    tic_list = []\n",
    "    for tic in selected_tics:\n",
    "        temp_df = df[df['ts_code'] == tic]\n",
    "        temp_long = temp_df['trade_date'].iloc[-1]\n",
    "        longth.append(temp_long)\n",
    "        tic_list.append(tic)\n",
    "#         print(tic_list,longth)\n",
    "    minlong = max(longth)\n",
    "    date_unique = [date for date in df['trade_date'].unique() if date >= minlong]\n",
    "    full_date_range = pd.DataFrame(date_unique,columns=['trade_date'])\n",
    "    \n",
    "    return full_date_range\n",
    "#     date_unique = df['trade_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k=50\n",
    "full_date_range = processed_date_range(df_ts,selected_tics,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669\n",
      "         ts_code trade_date   open   high    low  close  pre_close  change  \\\n",
      "0      601800.SH   20170103  15.13  15.39  15.13  15.29      15.19    0.10   \n",
      "1      601800.SH   20170104  15.24  15.60  15.11  15.45      15.29    0.16   \n",
      "2      601800.SH   20170105  15.40  15.65  15.18  15.42      15.45   -0.03   \n",
      "3      601800.SH   20170106  15.43  15.90  15.32  15.61      15.42    0.19   \n",
      "4      601800.SH   20170109  15.41  15.79  15.19  15.75      15.61    0.14   \n",
      "...          ...        ...    ...    ...    ...    ...        ...     ...   \n",
      "72945  600109.SH   20221226   8.68   8.73   8.48   8.52       8.69   -0.17   \n",
      "72946  600109.SH   20221227   8.60   8.72   8.55   8.68       8.52    0.16   \n",
      "72947  600109.SH   20221228   8.64   8.65   8.55   8.57       8.68   -0.11   \n",
      "72948  600109.SH   20221229   8.54   8.72   8.51   8.62       8.57    0.05   \n",
      "72949  600109.SH   20221230   8.64   8.73   8.64   8.70       8.62    0.08   \n",
      "\n",
      "       pct_chg        vol      amount  filled  \n",
      "0       0.6600  247541.17  378230.308       0  \n",
      "1       1.0500  262239.57  404372.419       0  \n",
      "2      -0.1900  239180.84  367547.886       0  \n",
      "3       1.2300  392150.44  615064.151       0  \n",
      "4       0.9000  284037.78  440851.265       0  \n",
      "...        ...        ...         ...     ...  \n",
      "72945  -1.9563  255110.22  217941.744       0  \n",
      "72946   1.8779  247763.62  214491.273       0  \n",
      "72947  -1.2673  113096.03   97101.726       0  \n",
      "72948   0.5834  147612.50  127139.208       0  \n",
      "72949   0.9281  144146.39  125225.155       0  \n",
      "\n",
      "[72950 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#遍历扩充股票的完整交易日期\n",
    "def add_exchange_calendars(df,full_date_range,selected_tics):\n",
    "    count = 0\n",
    "    merge_df = []\n",
    "    for tic in selected_tics:\n",
    "        temp_df = df[df['ts_code'] == tic]\n",
    "        temp_full_date_range = full_date_range\n",
    "        temp_df = temp_df.set_index('trade_date')\n",
    "        temp_full_date_range = temp_full_date_range.set_index('trade_date')\n",
    "        temp_df = pd.merge(temp_full_date_range,temp_df,how='left',left_index=True,right_index=True)\n",
    "        temp_df = temp_df.reset_index().sort_values('trade_date',ascending=True)\n",
    "        temp_df = temp_df.fillna({\n",
    "            'amount' : 0,\n",
    "            'vol': 0,\n",
    "            'pct_chg': 0,\n",
    "            'change':0,\n",
    "            'ts_code': tic\n",
    "        })\n",
    "        temp_df['filled'] = temp_df['close'].isna().astype(int)\n",
    "#         print(temp_df,temp_df['filled'].unique())\n",
    "        \n",
    "        for i in range(len(temp_df)):\n",
    "            if pd.isna(temp_df.loc[i,'close']):\n",
    "                j = i -1\n",
    "                while pd.isna(temp_df.loc[j,'close']):\n",
    "                    j = j - 1\n",
    "                if j > 0 :\n",
    "                    temp_df.loc[i,['close','open','high','low','pre_close']] = temp_df.loc[j,'close']\n",
    "                    count+=1\n",
    "        merge_df.append(temp_df)\n",
    "    merged_df = pd.concat(merge_df,ignore_index = True)\n",
    "    print(count)\n",
    "    return merged_df\n",
    "\n",
    "merged_df =  add_exchange_calendars(df_ts, full_date_range, selected_tics)\n",
    "# merged_df = merged_df.rename(columns={'trade_date':'date','ts_code':'tic','vol':'volume'})\n",
    "merged_df = merged_df[['ts_code','trade_date','open','high','low','close','pre_close','change','pct_chg','vol','amount','filled']]\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib as ta\n",
    "merged_df['ema'] = ta.EMA(merged_df[\"close\"], timeperiod=30)\n",
    "merged_df = merged_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>pre_close</th>\n",
       "      <th>change</th>\n",
       "      <th>pct_chg</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "      <th>filled</th>\n",
       "      <th>ema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601800.SH</td>\n",
       "      <td>20170103</td>\n",
       "      <td>15.13</td>\n",
       "      <td>15.39</td>\n",
       "      <td>15.13</td>\n",
       "      <td>15.29</td>\n",
       "      <td>15.19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>247541.17</td>\n",
       "      <td>378230.308</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>601800.SH</td>\n",
       "      <td>20170104</td>\n",
       "      <td>15.24</td>\n",
       "      <td>15.60</td>\n",
       "      <td>15.11</td>\n",
       "      <td>15.45</td>\n",
       "      <td>15.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.0500</td>\n",
       "      <td>262239.57</td>\n",
       "      <td>404372.419</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>601800.SH</td>\n",
       "      <td>20170105</td>\n",
       "      <td>15.40</td>\n",
       "      <td>15.65</td>\n",
       "      <td>15.18</td>\n",
       "      <td>15.42</td>\n",
       "      <td>15.45</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.1900</td>\n",
       "      <td>239180.84</td>\n",
       "      <td>367547.886</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>601800.SH</td>\n",
       "      <td>20170106</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.90</td>\n",
       "      <td>15.32</td>\n",
       "      <td>15.61</td>\n",
       "      <td>15.42</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2300</td>\n",
       "      <td>392150.44</td>\n",
       "      <td>615064.151</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>601800.SH</td>\n",
       "      <td>20170109</td>\n",
       "      <td>15.41</td>\n",
       "      <td>15.79</td>\n",
       "      <td>15.19</td>\n",
       "      <td>15.75</td>\n",
       "      <td>15.61</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>284037.78</td>\n",
       "      <td>440851.265</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72945</th>\n",
       "      <td>600109.SH</td>\n",
       "      <td>20221226</td>\n",
       "      <td>8.68</td>\n",
       "      <td>8.73</td>\n",
       "      <td>8.48</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.69</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-1.9563</td>\n",
       "      <td>255110.22</td>\n",
       "      <td>217941.744</td>\n",
       "      <td>0</td>\n",
       "      <td>8.769376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72946</th>\n",
       "      <td>600109.SH</td>\n",
       "      <td>20221227</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.72</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.68</td>\n",
       "      <td>8.52</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.8779</td>\n",
       "      <td>247763.62</td>\n",
       "      <td>214491.273</td>\n",
       "      <td>0</td>\n",
       "      <td>8.763610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72947</th>\n",
       "      <td>600109.SH</td>\n",
       "      <td>20221228</td>\n",
       "      <td>8.64</td>\n",
       "      <td>8.65</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8.68</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-1.2673</td>\n",
       "      <td>113096.03</td>\n",
       "      <td>97101.726</td>\n",
       "      <td>0</td>\n",
       "      <td>8.751119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72948</th>\n",
       "      <td>600109.SH</td>\n",
       "      <td>20221229</td>\n",
       "      <td>8.54</td>\n",
       "      <td>8.72</td>\n",
       "      <td>8.51</td>\n",
       "      <td>8.62</td>\n",
       "      <td>8.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5834</td>\n",
       "      <td>147612.50</td>\n",
       "      <td>127139.208</td>\n",
       "      <td>0</td>\n",
       "      <td>8.742660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72949</th>\n",
       "      <td>600109.SH</td>\n",
       "      <td>20221230</td>\n",
       "      <td>8.64</td>\n",
       "      <td>8.73</td>\n",
       "      <td>8.64</td>\n",
       "      <td>8.70</td>\n",
       "      <td>8.62</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>144146.39</td>\n",
       "      <td>125225.155</td>\n",
       "      <td>0</td>\n",
       "      <td>8.739907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72950 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ts_code trade_date   open   high    low  close  pre_close  change  \\\n",
       "0      601800.SH   20170103  15.13  15.39  15.13  15.29      15.19    0.10   \n",
       "1      601800.SH   20170104  15.24  15.60  15.11  15.45      15.29    0.16   \n",
       "2      601800.SH   20170105  15.40  15.65  15.18  15.42      15.45   -0.03   \n",
       "3      601800.SH   20170106  15.43  15.90  15.32  15.61      15.42    0.19   \n",
       "4      601800.SH   20170109  15.41  15.79  15.19  15.75      15.61    0.14   \n",
       "...          ...        ...    ...    ...    ...    ...        ...     ...   \n",
       "72945  600109.SH   20221226   8.68   8.73   8.48   8.52       8.69   -0.17   \n",
       "72946  600109.SH   20221227   8.60   8.72   8.55   8.68       8.52    0.16   \n",
       "72947  600109.SH   20221228   8.64   8.65   8.55   8.57       8.68   -0.11   \n",
       "72948  600109.SH   20221229   8.54   8.72   8.51   8.62       8.57    0.05   \n",
       "72949  600109.SH   20221230   8.64   8.73   8.64   8.70       8.62    0.08   \n",
       "\n",
       "       pct_chg        vol      amount  filled       ema  \n",
       "0       0.6600  247541.17  378230.308       0  0.000000  \n",
       "1       1.0500  262239.57  404372.419       0  0.000000  \n",
       "2      -0.1900  239180.84  367547.886       0  0.000000  \n",
       "3       1.2300  392150.44  615064.151       0  0.000000  \n",
       "4       0.9000  284037.78  440851.265       0  0.000000  \n",
       "...        ...        ...         ...     ...       ...  \n",
       "72945  -1.9563  255110.22  217941.744       0  8.769376  \n",
       "72946   1.8779  247763.62  214491.273       0  8.763610  \n",
       "72947  -1.2673  113096.03   97101.726       0  8.751119  \n",
       "72948   0.5834  147612.50  127139.208       0  8.742660  \n",
       "72949   0.9281  144146.39  125225.155       0  8.739907  \n",
       "\n",
       "[72950 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('1230_merged_ema.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1 = processed_df\n",
    "processed_df1 = processed_df1.sort_values(['ts_code','trade_date'],ascending=True,ignore_index=True)\n",
    "processed_df1['trade_date'] = pd.to_datetime(processed_df1['trade_date'])\n",
    "processed_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #检验50只股票的信息缺省的时间点和个数\n",
    "# dimatch_num = 0\n",
    "# dimatch_date =[]\n",
    "# for c in df_ts1['trade_date'].unique():\n",
    "#     temp = df_ts1[df_ts1['trade_date'] == c]\n",
    "#     if len(temp) != 50:\n",
    "#         dimatch_num +=1\n",
    "#         dimatch_date.append(c)\n",
    "# print(dimatch_num,dimatch_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_unique = df['date'].unique()\n",
    "# date_sum = len(date_unique)\n",
    "# count = 0\n",
    "# for date in date_unique:\n",
    "#     temp_df = df[df['trade_date'] == date]\n",
    "# #     if len(temp_df) = k:\n",
    "# #         count +=1\n",
    "# #     elif len(temp_df) < int(0.9*k):\n",
    "# #         date_unique.remove(date)\n",
    "# #     else:\n",
    "#     difference_tic = temp_df['ts_code'].tolist().difference(selected_tics)\n",
    "#     missing_tic.append(date,difference_tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #遍历扩充股票的完整交易日期\n",
    "# def add_exchange_calendars(df,full_date_range,selected_tics):\n",
    "#     merge_df = []\n",
    "#     for tic in selected_tics:\n",
    "#         temp_df = df[df['ts_code'] == tic]\n",
    "#         temp_full_date_range = full_date_range\n",
    "#         temp_df = temp_df.set_index('trade_date')\n",
    "#         temp_full_date_range = temp_full_date_range.set_index('trade_date')\n",
    "#         temp_df = pd.merge(temp_full_date_range,temp_df,how='left',left_index=True,right_index=True)\n",
    "#         temp_df = temp_df.reset_index().sort_values('trade_date',ascending=True)\n",
    "#         temp_df = temp_df.fillna({\n",
    "#             'amount' : 0,\n",
    "#             'vol': 0,\n",
    "#             'pct_chg': 0,\n",
    "#             'change':0,\n",
    "#             'ts_code': tic\n",
    "#         })\n",
    "#         for i in range(len(temp_df)):\n",
    "#             if pd.isna(temp_df.loc[i,'close']):\n",
    "#                 j = i -1\n",
    "#                 while pd.isna(temp_df.loc[j,'close']):\n",
    "#                     j = j - 1\n",
    "#                 if j > 0 :\n",
    "#                     temp_df.loc[i,['close','open','high','low','pre_close']] = temp_df.loc[j,'close']\n",
    "#         merge_df.append(temp_df)\n",
    "#     merged_df = pd.concat(merge_df,ignore_index = True)\n",
    "#     return merged_df\n",
    "\n",
    "# merged_df =  add_exchange_calendars(df_ts,full_date_range,selected_tics)\n",
    "# merged_df = merged_df.rename(columns={'trade_date':'date','ts_code':'tic','vol':'volume'})\n",
    "# merged_df = merged_df[['tic','date','open','high','low','close','pre_close','change','pct_chg','volume','amount']]\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 筛选exchange_calender中每个交易节点缺失情况\n",
    "# def processed_date(df,selected_tics,k):\n",
    "#     date_unique = list(df['trade_date'].unique())\n",
    "#     date_sum = len(date_unique)\n",
    "#     count = 0\n",
    "#     processe_df =[]\n",
    "#     processed_df = pd.DataFrame()\n",
    "#     for date in date_unique:\n",
    "#         temp_df = df[df['trade_date'] == date]\n",
    "#         if len(temp_df) < int(0.9*k):\n",
    "#             date_unique.remove(date)\n",
    "#         elif len(temp_df) >= int(0.9*k) and len(temp_df) < k:\n",
    "#             missing_tic = set(selected_tics).difference(set(temp_df['ts_code'].tolist()))\n",
    "# #             print(missing_tic)\n",
    "#             for m_tic in missing_tic:\n",
    "#                 temp_dict = {'ts_code':m_tic,'trade_date':date,'open':np.nan, 'high':np.nan,'low':np.nan, 'close':np.nan, 'pre_close':np.nan, 'change':np.nan, 'pct_chg':np.nan, 'volume':np.nan, 'amount':np.nan}\n",
    "#                 new_data = pd.DataFrame.from_dict(temp_dict,orient='index').T\n",
    "#                 temp_df = temp_df.append(new_data)\n",
    "# #             print(len(temp_df))\n",
    "#             processe_df.append(temp_df)\n",
    "#         else :\n",
    "#             count += 1\n",
    "#             processe_df.append(temp_df)\n",
    "#         processed_df = pd.concat(processe_df,ignore_index = True)       \n",
    "#     processed_df = processed_df.sort_values('trade_date',ascending=True).reset_index()\n",
    "#     processed_df = processed_df.drop('index',axis=1)\n",
    "#     return processed_df,date_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 筛选exchange_calender中每个交易节点缺失情况(全部补充0)\n",
    "# def processed_date(df,selected_tics,k):\n",
    "#     date_unique = list(df['trade_date'].unique())\n",
    "#     date_sum = len(date_unique)\n",
    "#     count = 0\n",
    "#     processe_df =[]\n",
    "#     processed_df = pd.DataFrame()\n",
    "#     for date in date_unique:\n",
    "#         temp_df = df[df['trade_date'] == date]\n",
    "#         if len(temp_df) != k:\n",
    "#             missing_tic = set(selected_tics).difference(set(temp_df['ts_code'].tolist()))\n",
    "# #             print(missing_tic)\n",
    "#             for m_tic in missing_tic:\n",
    "#                 temp_dict = {'ts_code':m_tic,'trade_date':date,'open':0, 'high':0,'low':0, 'close':0, 'pre_close':0, 'change':0, 'pct_chg':0, 'volume':0, 'amount':0}\n",
    "#                 new_data = pd.DataFrame.from_dict(temp_dict,orient='index').T\n",
    "#                 temp_df = temp_df.append(new_data)\n",
    "#             processe_df.append(temp_df)\n",
    "#         else :\n",
    "#             processe_df.append(temp_df)\n",
    "#         processed_df = pd.concat(processe_df,ignore_index = True)       \n",
    "#     processed_df = processed_df.sort_values('trade_date',ascending=True).reset_index()\n",
    "#     processed_df = processed_df.drop('index',axis=1)\n",
    "#     return processed_df,date_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 筛选exchange_calender中每个交易节点缺失(激进)\n",
    "# def processed_date(df,selected_tics,k):\n",
    "#     date_unique = list(df['trade_date'].unique())\n",
    "#     date_sum = len(date_unique)\n",
    "#     count = 0\n",
    "#     processe_df =[]\n",
    "#     processed_df = pd.DataFrame()\n",
    "#     for date in date_unique:\n",
    "#         temp_df = df[df['trade_date'] == date]\n",
    "#         if len(temp_df) == k:\n",
    "#             processe_df.append(temp_df)\n",
    "#         else :\n",
    "#             date_unique.remove(date)\n",
    "#         processed_df = pd.concat(processe_df,ignore_index = True)       \n",
    "#     processed_df = processed_df.sort_values('trade_date',ascending=True).reset_index()\n",
    "#     processed_df = processed_df.drop('index',axis=1)\n",
    "#     return processed_df,date_unique\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df,date_unique = processed_date(df_ts,selected_tics,k)\n",
    "# print(processed_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df1 = processed_df.drop('volume',axis=1)\n",
    "# processed_df1 = processed_df1.sort_values(['ts_code','trade_date'],ascending=True,ignore_index=True)\n",
    "# processed_df1['trade_date'] = pd.to_datetime(processed_df1['trade_date'])\n",
    "# processed_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = INDICATORS,\n",
    "                    use_vix=False,\n",
    "                    use_turbulence=False,\n",
    "                    user_defined_feature = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_df3 = processed_df1.rename(columns={'ts_code':'tic','trade_date':'date','vol':'volume'})\n",
    "processed_df3 = fe.preprocess_data(processed_df3)\n",
    "processed_df3 = processed_df3.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "processed_df3 = processed_df3.sort_values(['tic','date'],ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delaydate(dela,full_list):\n",
    "    grouped = processed_df3.groupby('tic')\n",
    "    filtered_df = pd.DataFrame()\n",
    "    for name, group in grouped:\n",
    "        # 按'trade_date'升序排序  True=升序\n",
    "        group = group.sort_values('date', ascending=True)\n",
    "\n",
    "        # 丢弃前48个日期的数据\n",
    "        group = group.iloc[dela:]\n",
    "        filtered_df = filtered_df.append(group)\n",
    "    \n",
    "    filtered_df = filtered_df.reset_index(drop=True)\n",
    "    filtered_df = filtered_df.sort_values(['tic','date'],ascending=True,ignore_index=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_df = processed_df1.loc[:,['open','high','low','close','vol']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#出现缺失值的个数、占比\n",
    "count = len(pro_df[pro_df['open'] == 0])\n",
    "nan_rate = count/len(pro_df)\n",
    "print(count,'缺失值百分之:',nan_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算技术指标到dataset中\n",
    "#Can be easily expanded\n",
    "#Currently contains a small set of tech indicators\n",
    "import talib as ta\n",
    "\n",
    "def calc_tech_ind(data):\n",
    "    #overlap \n",
    "    data['upbd'], data['midbd'], data['lowbd'] = ta.BBANDS(data[\"close\"])\n",
    "    data['dema'] = ta.DEMA(data[\"close\"], timeperiod=30)\n",
    "    data['tema'] = ta.TEMA(data[\"close\"], timeperiod=30)\n",
    "#     data['ema'] = ta.EMA(data[\"close\"], timeperiod=30)\n",
    "#     data['wma'] = ta.WMA(data[\"close\"], timeperiod=30)\n",
    "#     data['sma'] = ta.SMA(data[\"close\"], timeperiod=30)\n",
    "#     data['sarext'] = ta.SAREXT(data[\"high\"], data[\"low\"])\n",
    "    \n",
    "    #momentum\n",
    "    data['adxr'] = ta.ADXR(data[\"high\"], data[\"low\"], data[\"close\"], timeperiod=14)\n",
    "    data['apo'] = ta.APO(data[\"close\"], fastperiod=12, slowperiod=26, matype=0)\n",
    "    data['aroondown'], data['aroonup'] = ta.AROON(data[\"high\"], data[\"low\"], timeperiod=14)\n",
    "    data['cci'] = ta.CCI(data[\"high\"], data[\"low\"], data[\"close\"], timeperiod=14)\n",
    "    data['cmo'] = ta.CMO(data[\"close\"], timeperiod=14)\n",
    "    data['macd'], data['macdsignal'], data['macdhist'] = ta.MACD(data[\"close\"], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    data['MFI'] = ta.MFI(data[\"high\"], data[\"low\"], data[\"close\"], data['vol'], timeperiod=14)\n",
    "#     data['mom'] = ta.MOM(data[\"close\"], timeperiod=10)\n",
    "#     data['plus_di'] = ta.PLUS_DI(data[\"high\"], data[\"low\"], data[\"close\"], timeperiod=14)\n",
    "#     data['ppo'] = ta.PPO(data[\"close\"], fastperiod=12, slowperiod=26, matype=0)\n",
    "#     data['roc'] = ta.ROC(data[\"close\"], timeperiod=10)\n",
    "#     data['rocp'] = ta.ROCP(data[\"close\"], timeperiod=10)\n",
    "#     data['rsi'] = ta.RSI(data[\"close\"], timeperiod=14)\n",
    "#     data['slowk'], data['slowd'] = ta.STOCH(data[\"high\"], data[\"low\"], data[\"close\"])\n",
    "#     data['fastk'], data['fastd'] = ta.STOCHF(data[\"high\"], data[\"low\"], data[\"close\"])\n",
    "#     data['trix'] = ta.TRIX(data[\"close\"], timeperiod=30)\n",
    "#     data['ultosc'] = ta.ULTOSC(data[\"high\"], data[\"low\"], data[\"close\"], timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "#     data['willr'] = ta.WILLR(data[\"high\"], data[\"low\"], data[\"close\"], timeperiod=14)\n",
    "    \n",
    "    #volume\n",
    "    data['ad'] = ta.AD(data[\"high\"], data[\"low\"], data[\"close\"], data['vol'])\n",
    "    data['obv'] = ta.OBV(data[\"close\"], data['vol'])\n",
    "    \n",
    "    #volitility\n",
    "    data['atr'] = ta.ATR(data[\"high\"], data[\"low\"], data[\"close\"], timeperiod=14)\n",
    "    data['natr'] = ta.NATR(data[\"high\"], data[\"low\"], data[\"close\"], timeperiod=14)\n",
    "    \n",
    "    #cycle\n",
    "    data['HT_DCPERIOD'] = ta.HT_DCPERIOD(data[\"close\"])\n",
    "#     data['HT_DCPHASE'] = ta.HT_DCPHASE(data[\"close\"])\n",
    "#     data['inphase'], data['quadrature'] = ta.HT_PHASOR(data[\"close\"])\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list1 = calc_tech_ind(pro_df)\n",
    "full_list1 = full_list1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list2 = calc_tech_ind(backtest_df_reduction)\n",
    "full_list2 = full_list2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_full_list = np.array(full_list1)\n",
    "array_full_list = array_full_list.reshape(k,int(len(full_list1)/k),full_list1.shape[1])\n",
    "selected_tics = sorted(selected_tics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(array_full_list[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(array_full_list[:,:,3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_full_list2 = np.array(full_list2)\n",
    "array_full_list2 = array_full_list2.reshape(k,int(len(full_list2)/k),full_list2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(array_full_list).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text\n",
    "# tdata = array_full_list[0][:-1]\n",
    "ttdata = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "windows = 5\n",
    "gap = 1\n",
    "batch_size = 3\n",
    "start = len(ttdata) - windows\n",
    "test_list = []\n",
    "while start >= 0:\n",
    "#     print(ttdata[start:start + windows - 1],ttdata[start + windows-1])\n",
    "    segdata = ttdata[start:start + windows]\n",
    "#     segclose = ttdata[start:start + windows]\n",
    "    test_list.extend([segdata] * batch_size)\n",
    "#     test_list.append(segdata)\n",
    "    start = start - gap\n",
    "    print(test_list,'hhhhhhhhhhhhhhh',segdata)\n",
    "    \n",
    "# print(np.array(test_list)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #generate x, y, z, zp quadruples\n",
    "# #segment x, y, z trios to sequence according to $timeStep and $gap\n",
    "# #x: historical data w/ technical analysis indicator\n",
    "# #y: closing price of t+1\n",
    "# #z:  difference between t+1 and t step's closing price\n",
    "\n",
    "# def toSequential(idx, full_list, timeStep=48, gap=4):\n",
    "#     #closing: from id=0 to last\n",
    "#     closing=full_list[idx][:, 3]\n",
    "#     #data from id=0 to second to last\n",
    "#     data=full_list[idx][:-1]\n",
    "#     #calculating number of available sequential samples\n",
    "#     data_length=len(data)\n",
    "#     count=(data_length-timeStep)//gap+1\n",
    "#     stockSeq=[]\n",
    "#     labelSeq=[]\n",
    "#     diffSeq=[]\n",
    "#     realDiffSeq=[]\n",
    "    \n",
    "#     start = data_length - timeStep\n",
    "#     while start >= 0:\n",
    "#         segData = data[start:start + timeStep - 1]\n",
    "#         segClosing = closing[start:start + timeStep]\n",
    "#         std_dev = segData.std(axis=0, keepdims=True)\n",
    "#         std_dev_nonzero = np.where(std_dev == 0, 1, std_dev)  # 处理分母为零的情况\n",
    "#         segDataNorm = np.nan_to_num((segData - segData.mean(axis=0, keepdims=True)) / std_dev_nonzero)\n",
    "#         std_dev_close = segClosing.std()\n",
    "#         std_dev_close_nonzero = np.where(std_dev_close == 0, 1, std_dev_close)\n",
    "#         segClosingNorm=(segClosing-segClosing.mean())/std_dev_close_nonzero\n",
    "#         stockSeq.append(segDataNorm)\n",
    "#         labelSeq.append(segClosingNorm[1:])\n",
    "# #         print(np.isnan(labelSeq).any())\n",
    "#         diffSeq.append(segClosingNorm[1:]-segClosingNorm[:-1])\n",
    "#         realDiffSeq.append(segClosing[1:]-segClosing[:-1])       \n",
    "#         start = start - gap\n",
    "#     stockSeq=np.array(stockSeq)[::-1]\n",
    "#     labelSeq=np.array(labelSeq)[::-1]\n",
    "#     diffSeq=np.array(diffSeq)[::-1]\n",
    "#     realDiffSeq=np.array(realDiffSeq)[::-1]\n",
    "    \n",
    "#     return stockSeq.astype('float32') , labelSeq.astype('float32'), diffSeq.astype('float32'), realDiffSeq.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入是 50 * feature_size   full_list的形状是(full_date_range,k,feature_size)\n",
    "\n",
    "def toSequential_train(full_list, seq_len = 48 , gap = 1):\n",
    "    closing = full_list[:,:,3]\n",
    "    data = full_list\n",
    "    data_length = len(data)\n",
    "    count = (data_length - seq_len) // gap + 1\n",
    "    stockSeq=[]\n",
    "    labelSeq=[]\n",
    "    diffSeq=[]\n",
    "    realDiffSeq=[]\n",
    "    \n",
    "    for i in range(count):\n",
    "        #segData dims: [timestep, feature count]\n",
    "        \n",
    "        segData = data[gap * i: gap * i + seq_len]\n",
    "        segClosing = closing[gap * i: gap * i + seq_len + 1]\n",
    "        \n",
    "        #segDiff=diff[gap*i:gap*i+timeStep]\n",
    "        #normalization\n",
    "        \n",
    "        std_dev = segData.std(axis=0, keepdims=True)\n",
    "        std_dev_nonzero = np.where(std_dev == 0, 1, std_dev)  # 处理分母为零的情况\n",
    "        \n",
    "#         segDataNorm = np.nan_to_num((segData - segData.mean(axis=0, keepdims=True)) / std_dev_nonzero)\n",
    "        segDataNorm = np.nan_to_num((segData-segData.mean(axis=0, keepdims=True))/segData.std(axis=0, keepdims=True))\n",
    "\n",
    "        std_close = segClosing.std()\n",
    "        std_segClosing = np.where(std_close == 0, 1, std_close)\n",
    "        segClosingNorm=(segClosing-segClosing.mean())/std_segClosing\n",
    "#         segDiff=(segDiff-segDiff.mean())/segDiff.std()\n",
    "        \n",
    "        stockSeq.append(segDataNorm)\n",
    "        labelSeq.append(segClosingNorm[1:])\n",
    "        diffSeq.append(segClosingNorm[1:]-segClosingNorm[:-1])\n",
    "        realDiffSeq.append(segClosing[1:]-segClosing[:-1])\n",
    "        print(len(labelSeq))\n",
    "    stockSeq=np.array(stockSeq)\n",
    "    labelSeq=np.array(labelSeq)\n",
    "    diffSeq=np.array(diffSeq)\n",
    "    realDiffSeq=np.array(realDiffSeq)\n",
    "    return stockSeq.astype('float32') , labelSeq.astype('float32'), diffSeq.astype('float32'), realDiffSeq.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSequential_back(full_list, seq_len = 48, gap = 1):\n",
    "    closing = full_list[:,:,3]\n",
    "    data = full_list\n",
    "    data_length = len(data)\n",
    "    count = (data_length - seq_len) // gap + 1\n",
    "    stockSeq=[]\n",
    "    labelSeq=[]\n",
    "    diffSeq=[]\n",
    "    realDiffSeq=[]\n",
    "    \n",
    "    start = data_length - seq_len\n",
    "    i=0\n",
    "    while start >= 0:\n",
    "        segData = data[start:start + seq_len]\n",
    "        std_dev = segData.std(axis=0, keepdims=True)\n",
    "        std_dev_nonzero = np.where(std_dev == 0, 1, std_dev)  # 处理分母为零的情况\n",
    "        segDataNorm = np.nan_to_num((segData - segData.mean(axis=0, keepdims=True)) / std_dev_nonzero)\n",
    "        #复制batch_size个\n",
    "        stockSeq.append(segDataNorm)\n",
    "#         stockSeq.extend([segDataNorm] * batch_size)\n",
    "#         stockSeq.append(segDataNorm)   \n",
    "        start = start - gap\n",
    "        i +=1\n",
    "    stockSeq=np.array(stockSeq)[::-1]\n",
    "#     print(i)\n",
    "    return stockSeq.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input each step:  vector including [stock info, tech indicators]\n",
    "#output each step: closing price t+1, price diff between t+1 and t\n",
    "#full_list: output from get_data_set\n",
    "\n",
    "class StockDataset_back(Dataset):\n",
    "    def __init__(self, full_list, seq_len = 48, gap = 1):\n",
    "\n",
    "        X = toSequential_back(full_list, seq_len = seq_len, gap = gap)\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data=self.X[index]\n",
    "        label1=self.y[index]\n",
    "\n",
    "        return (data, label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input each step:  vector including [stock info, tech indicators]\n",
    "#output each step: closing price t+1, price diff between t+1 and t\n",
    "#full_list: output from get_data_set\n",
    "\n",
    "class StockDataset_train(Dataset):\n",
    "    def __init__(self, split_rate, full_list, seq_len = 48, gap=3):\n",
    "\n",
    "        X, y, z, zp=toSequential_train(full_list, seq_len = seq_len, gap = gap)\n",
    "\n",
    "        self.X = X[:int(len(X)*split_rate),:,:]\n",
    "        self.y = y[:int(len(y)*split_rate),:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data=self.X[index]\n",
    "        label1=self.y[index]\n",
    "\n",
    "        return (data, label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset_val(Dataset):\n",
    "    def __init__(self, split_rate, full_list, seq_len=24, gap=1):\n",
    "\n",
    "        X, y, z, zp=toSequential_train(full_list, seq_len = seq_len, gap = gap)\n",
    "\n",
    "        self.X = X[int(len(X)*split_rate):,:,:]\n",
    "        self.y = y[int(len(y)*split_rate):,:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data=self.X[index]\n",
    "        label1=self.y[index]\n",
    "\n",
    "        return (data, label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_iter = DataLoader(StockDataset_val(split_rate = 0.7,full_list = array_full_list), shuffle=True, batch_size=64, num_workers=0,drop_last=True)\n",
    "train_iter = DataLoader(StockDataset_train(split_rate = 0.7,full_list = array_full_list), shuffle=True, batch_size=64, num_workers=0,drop_last=True)\n",
    "back_iter = DataLoader(StockDataset_back(full_list = array_full_list), shuffle=False, batch_size=64, num_workers=0,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate x, y, z, zp quadruples\n",
    "#segment x, y, z trios to sequence according to $timeStep and $gap\n",
    "#x: historical data w/ technical analysis indicator\n",
    "#y: closing price of t+1\n",
    "#z:  difference between t+1 and t step's closing price\n",
    "\n",
    "def toSequential_train1(idx, full_list, timeStep=48, gap=4):\n",
    "    #closing: from id=0 to last\n",
    "    closing=full_list[idx][:, 3]\n",
    "#     closingNorm = (closing - closing.mean())/closing.std()\n",
    "    #data from id=0 to second to last\n",
    "    data=full_list[idx][:-1]\n",
    "    #calculating number of available sequential samples\n",
    "    data_length=len(data)\n",
    "    count=(data_length-timeStep)//gap+1\n",
    "    stockSeq=[]\n",
    "    labelSeq=[]\n",
    "    diffSeq=[]\n",
    "    realDiffSeq=[]\n",
    "    for i in range(count):\n",
    "        #segData dims: [timestep, feature count]       \n",
    "        segData=data[gap*i:gap*i+timeStep]\n",
    "        segClosing=closing[gap*i:gap*i+timeStep+1]\n",
    "        #segDiff=diff[gap*i:gap*i+timeStep]\n",
    "        #normalization\n",
    "        \n",
    "        std_dev = segData.std(axis=0, keepdims=True)\n",
    "        std_dev_nonzero = np.where(std_dev == 0, 1, std_dev)  # 处理分母为零的情况\n",
    "#         segDataNorm = np.nan_to_num((segData - segData.mean(axis=0, keepdims=True)) / std_dev_nonzero)\n",
    "        segDataNorm=np.nan_to_num((segData-segData.mean(axis=0, keepdims=True))/segData.std(axis=0, keepdims=True))\n",
    "    \n",
    "        std_close = segClosing.std()\n",
    "        std_segClosing = np.where(std_close == 0, 1, std_close)\n",
    "        segClosingNorm=(segClosing-segClosing.mean())/std_segClosing\n",
    "#         segDiff=(segDiff-segDiff.mean())/segDiff.std()\n",
    "        \n",
    "        stockSeq.append(segDataNorm)\n",
    "        labelSeq.append(segClosingNorm[1:])\n",
    "        diffSeq.append(segClosingNorm[1:]-segClosingNorm[:-1])\n",
    "        realDiffSeq.append(segClosing[1:]-segClosing[:-1])\n",
    "    stockSeq=np.array(stockSeq)\n",
    "    labelSeq=np.array(labelSeq)\n",
    "    diffSeq=np.array(diffSeq)\n",
    "    realDiffSeq=np.array(realDiffSeq)\n",
    "    return stockSeq.astype('float32') , labelSeq.astype('float32'), diffSeq.astype('float32'), realDiffSeq.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input each step:  vector including [stock info, tech indicators]\n",
    "#output each step: closing price t+1, price diff between t+1 and t\n",
    "#full_list: output from get_data_set\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, id_list, full_list, transform=None, timestep=48, gap=3):\n",
    "        self.transform=transform\n",
    "        self.id_list=id_list\n",
    "        \n",
    "        stock_cohort=[]\n",
    "        closing_cohort=[]\n",
    "        diff_cohort=[]\n",
    "        real_diff_cohort=[]\n",
    "        \n",
    "        #load data into cohort\n",
    "        for i in self.id_list:\n",
    "            X, y, z, zp=toSequential_train1(i, full_list, timeStep=timestep, gap=gap)\n",
    "            stock_cohort.append(X)\n",
    "            closing_cohort.append(y)\n",
    "            diff_cohort.append(z)\n",
    "            real_diff_cohort.append(zp)\n",
    "        self.X=np.concatenate(stock_cohort, axis=0)\n",
    "        self.y=np.concatenate(closing_cohort, axis=0)\n",
    "        self.z=np.concatenate(diff_cohort, axis=0)  \n",
    "        self.zp=np.concatenate(real_diff_cohort, axis=0)\n",
    "        print(self.X)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        data returned in the format of \n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx=idx.tolist()\n",
    "        \n",
    "        data=self.X[idx]\n",
    "        label1=self.y[idx]\n",
    "        label2=self.z[idx]\n",
    "        label3=self.zp[idx]\n",
    "        if self.transform:\n",
    "            data=self.transform(data)\n",
    "        return (data, label1, label2, label3)\n",
    "    \n",
    "    \n",
    "    def getDS(self):\n",
    "        return self.X, self.y, self.z, self.zp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate x, y, z, zp quadruples\n",
    "#segment x, y, z trios to sequence according to $timeStep and $gap\n",
    "#x: historical data w/ technical analysis indicator\n",
    "#y: closing price of t+1\n",
    "#z:  difference between t+1 and t step's closing price\n",
    "\n",
    "def toSequential_train(idx, full_list, timeStep=48, gap=4):\n",
    "    #closing: from id=0 to last\n",
    "    closing=full_list[idx][:, 3]\n",
    "    closingNorm = (closing - closing.mean())/closing.std()\n",
    "    #data from id=0 to second to last\n",
    "    data=full_list[idx][:-1]\n",
    "    #calculating number of available sequential samples\n",
    "    data_length=len(data)\n",
    "    count=(data_length-timeStep)//gap+1\n",
    "    stockSeq=[]\n",
    "    labelSeq=[]\n",
    "    diffSeq=[0,]\n",
    "    realDiffSeq=[0,]\n",
    "    for i in range(count-1):\n",
    "        #segData dims: [timestep, feature count]       \n",
    "        segData=data[gap*i:gap*i+timeStep]\n",
    "        segClosing=closingNorm[gap*i+timeStep]\n",
    "        #segDiff=diff[gap*i:gap*i+timeStep]\n",
    "        #normalization\n",
    "        \n",
    "        std_dev = segData.std(axis=0, keepdims=True)\n",
    "        std_dev_nonzero = np.where(std_dev == 0, 1, std_dev)  # 处理分母为零的情况\n",
    "        segDataNorm = np.nan_to_num((segData - segData.mean(axis=0, keepdims=True)) / std_dev_nonzero)\n",
    "#       segDataNorm=np.nan_to_num((segData-segData.mean(axis=0, keepdims=True))/segData.std(axis=0, keepdims=True))\n",
    "#       segClosingNorm=(segClosing-segClosing.mean())/segClosing.std()\n",
    "        #segDiff=(segDiff-segDiff.mean())/segDiff.std()\n",
    "        \n",
    "        stockSeq.append(segDataNorm)\n",
    "        labelSeq.append(segClosing)\n",
    "        diffSeq.append(segClosing-diffSeq[-1])\n",
    "        realDiffSeq.append(closing[gap*i+timeStep]-realDiffSeq[-1])\n",
    "    stockSeq=np.array(stockSeq)\n",
    "    labelSeq=np.array(labelSeq)\n",
    "    diffSeq=np.array(diffSeq)\n",
    "    realDiffSeq=np.array(realDiffSeq)\n",
    "    return stockSeq.astype('float32') , labelSeq.astype('float32'), diffSeq.astype('float32'), realDiffSeq.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSequential_back(idx, full_list, timeStep = 48, gap = 1,batch_size = 64):\n",
    "    #closing: from id=0 to last\n",
    "    closing=full_list[idx][:, 3]\n",
    "    #data from id=0 to second to last\n",
    "    data=full_list[idx][:-1]\n",
    "    #calculating number of available sequential samples\n",
    "    data_length=len(data)\n",
    "    count=(data_length-timeStep)//gap+1\n",
    "    stockSeq=[]\n",
    "    labelSeq=[]\n",
    "    diffSeq=[]\n",
    "    realDiffSeq=[]\n",
    "    \n",
    "    start = data_length - timeStep\n",
    "    i=0\n",
    "    while start >= 0:\n",
    "        segData = data[start:start + timeStep]\n",
    "        std_dev = segData.std(axis=0, keepdims=True)\n",
    "        std_dev_nonzero = np.where(std_dev == 0, 1, std_dev)  # 处理分母为零的情况\n",
    "        segDataNorm = np.nan_to_num((segData - segData.mean(axis=0, keepdims=True)) / std_dev_nonzero)\n",
    "        #复制batch_size个\n",
    "        stockSeq.extend([segDataNorm] * batch_size)\n",
    "#         stockSeq.append(segDataNorm)   \n",
    "        start = start - gap\n",
    "        i +=1\n",
    "    stockSeq=np.array(stockSeq)[::-1]\n",
    "#     print(i)\n",
    "    return stockSeq.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input each step:  vector including [stock info, tech indicators]\n",
    "#output each step: closing price t+1, price diff between t+1 and t\n",
    "#full_list: output from get_data_set\n",
    "\n",
    "class StockDataset_back(Dataset):\n",
    "    def __init__(self, id_list, full_list, transform=None, timestep=48, gap=1):\n",
    "        self.transform=transform\n",
    "        self.id_list=id_list\n",
    "        \n",
    "        stock_cohort=[]\n",
    "        \n",
    "        #load data into cohort\n",
    "        for i in self.id_list:\n",
    "            X=toSequential_back(i, full_list, timeStep=timestep, gap=gap)\n",
    "            stock_cohort.append(X)\n",
    "        self.X=np.concatenate(stock_cohort, axis=0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        data returned in the format of \n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx=idx.tolist()\n",
    "        \n",
    "        data=self.X[idx]\n",
    "        if self.transform:\n",
    "            data=self.transform(data)\n",
    "        return data\n",
    "        \n",
    "    def getDS(self):\n",
    "        return self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation of training, validation, and testing dataset\n",
    "def DataIterGen(test_id_list, val_id_list, name_list, full_list, demo=False):\n",
    "    \"\"\"\n",
    "    test_id_list: id of subjects for testing\n",
    "    val_id_list: id of subjects for validation\n",
    "    other subjects for training\n",
    "    full_list=get_data_set(name_list), preprocessed\n",
    "    demo: when demo mode is True, only test_iter is returned, with data from\n",
    "    first entry of test_id_list (single stock)\n",
    "    \"\"\"\n",
    "    name_count=len(name_list)\n",
    "\n",
    "    if demo:\n",
    "        test_iter=DataLoader(StockDataset(test_id_list[0:1], full_list, timestep=24, gap=1), shuffle=False, batch_size=64, num_workers=0)\n",
    "        print(f'Demo with stock: {name_list[test_id_list[0]]} ')\n",
    "        return test_iter\n",
    "    else:\n",
    "        all_ids = list(range(name_count))\n",
    "        train_id_list = list(set(all_ids) - set(test_id_list) - set(val_id_list))\n",
    "#         partial_list=full_list[train_list,:,:]\n",
    "        test_iter=DataLoader(StockDataset(test_id_list, full_list), batch_size=64, num_workers=0,drop_last=True)\n",
    "        val_iter=DataLoader(StockDataset_back(val_id_list, full_list), batch_size=64, num_workers=0,drop_last=True)\n",
    "        train_iter=DataLoader(StockDataset(train_id_list, full_list), shuffle=True, batch_size=64, num_workers=0,drop_last=True)\n",
    "        print(f'Val: {[name_list[val_id] for val_id in val_id_list]}, Test: {[name_list[test_id] for test_id in test_id_list]}, Train: {[name_list[train_id] for train_id in train_id_list]} ')\n",
    "        return train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = [1,2,3,4,5,6,7]\n",
    "rate = 0.7\n",
    "print(mm[:int(len(mm)*rate)],mm[int(len(mm)*rate):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = DataIterGen([3,4,5,9,10],[7,11,13,14],selected_tics,array_full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(array_full_list[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, d_model, d_ff, num_heads,env_size, num_layers,dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.encode = Encoder(d_model, d_ff, num_heads, num_layers,dropout_rate)\n",
    "        self.linear1 = nn.Linear(d_model, env_size)\n",
    "        self.rule = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(env_size,1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "#         print(x.shape)\n",
    "        x = x.permute(1,0,2)\n",
    "        encoded = self.encode(x)\n",
    "        encoded = self.linear1(encoded)\n",
    "        x = self.rule(encoded)\n",
    "        x = self.linear2(x)\n",
    "#         print(x.shape)\n",
    "        decoded = x.permute(1,0,2)\n",
    "        return decoded, encoded.permute(1,0,2)[0,-1,:]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads, num_layers,dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, d_ff, num_heads, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads,dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, num_heads)\n",
    "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # 保存输入的残差连接\n",
    "        encoder_mask = torch.triu(torch.ones(x.size(0), x.size(0)), diagonal=1).bool().to(device)\n",
    "        # Self-Attention\n",
    "        x, _ = self.self_attention(x, x, x,attn_mask=encoder_mask)\n",
    "#         x = self.dropout1(x)\n",
    "        x = x + residual  # 残差连接\n",
    "        x = self.norm1(x)  # Add & Norm\n",
    "\n",
    "        residual = x  # 保存 Self-Attention 后的残差连接\n",
    "\n",
    "        # Feed-Forward\n",
    "        x = self.feed_forward(x)\n",
    "        x = x + residual  # 残差连接\n",
    "        x = self.norm2(x)  # Add & Norm\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout_rate):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型训练\n",
    "def train(model, train_iter, optimizer, num_epochs): \n",
    "    # 训练循环\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        enVec_list = []\n",
    "        for X, y, z, zp in train_iter:  \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = X.to(device)\n",
    "            targets = y.unsqueeze(2).to(device)\n",
    "            pred,enVec = model(inputs)\n",
    "            enVec_list.append(enVec)\n",
    "#             print(pred,targets)\n",
    "            loss = nn.MSELoss()(pred, targets)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        train_losses.append(total_loss/len(train_iter))\n",
    "        # 打印每个 epoch 的损失\n",
    "        print(f\"Epoch {epoch+1}: Loss: {total_loss/len(train_iter):.4f}\")\n",
    "    %matplotlib inline    \n",
    "        # 绘制损失变化曲线\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return enVec_list\n",
    "\n",
    "# 模型评估\n",
    "def val(model, val_iter):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    enVec_list = []\n",
    "    with torch.no_grad():\n",
    "        for X, y, z, zp in val_iter: \n",
    "            inputs = X.to(device)\n",
    "            targets = y.unsqueeze(2).to(device)\n",
    "            pred, enVec = model(inputs)\n",
    "            enVec_list.append(enVec)\n",
    "            loss = nn.MSELoss()(pred, targets)  \n",
    "            total_loss += loss.item()\n",
    "    print(f\"val Loss: {total_loss/len(val_iter):.4f}\")\n",
    "    return enVec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型训练\n",
    "def train(model, train_iter, optimizer, num_epochs): \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        enVec_list = []\n",
    "        predictions = []  # 保存每个epoch的一些样本的预测值\n",
    "        targets_list = []  # 保存每个epoch的一些样本的真实值\n",
    "        for X, y, z, zp in train_iter:  \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = X.to(device)\n",
    "            targets = y.unsqueeze(2).to(device)\n",
    "            pred, enVec = model(inputs)\n",
    "            enVec_list.append(enVec)\n",
    "            \n",
    "            # 保存一些样本的预测值和真实值\n",
    "            if np.random.rand() < 0.1:  # 以10%的概率保存样本\n",
    "                predictions.append(pred.cpu().detach().numpy())\n",
    "                targets_list.append(targets.cpu().detach().numpy())\n",
    "            \n",
    "            loss = nn.MSELoss()(pred, targets)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # 打印每个 epoch 的损失\n",
    "        print(f\"Epoch {epoch+1}: Loss: {total_loss/len(train_iter):.4f}\")\n",
    "        \n",
    "        # 绘制一些样本的预测值与真实值图\n",
    "        if epoch % 5 == 0:  # 每5个epoch绘制一次\n",
    "            plot_predictions(targets_list, predictions, epoch)\n",
    "\n",
    "    return enVec_list\n",
    "\n",
    "# 模型评估\n",
    "def val(model, val_iter):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    enVec_list = []\n",
    "    with torch.no_grad():\n",
    "        for X, y, z, zp in val_iter: \n",
    "            inputs = X.to(device)\n",
    "            targets = y.unsqueeze(2).to(device)\n",
    "            pred, enVec = model(inputs)\n",
    "            enVec_list.append(enVec)\n",
    "            loss = nn.MSELoss()(pred, targets)  \n",
    "            total_loss += loss.item()\n",
    "    print(f\"val Loss: {total_loss/len(val_iter):.4f}\")\n",
    "    return enVec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(targets_list, predictions, epoch):\n",
    "    targets = np.concatenate(targets_list, axis=0)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(min(5, targets.shape[0])):  # 绘制前5个样本\n",
    "        plt.subplot(5, 1, i+1)\n",
    "        plt.plot(targets[i], label='True')\n",
    "        plt.plot(predictions[i], label='Predicted')\n",
    "        plt.title(f'Sample {i+1} - Epoch {epoch+1}')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,back_iter):\n",
    "    enVec_list = []\n",
    "    with torch.no_grad():\n",
    "        for X in back_iter:\n",
    "            inputs = X.to(device)\n",
    "            _,enVec = model(inputs)\n",
    "            enVec_list.append(enVec)\n",
    "    return enVec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define device\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "device=try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "model = Transformer(input_size = 25, d_model = 64 , num_layers = 2, d_ff = 128 ,num_heads = 8,env_size =20,dropout_rate=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "outcoming =train(model,train_iter,optimizer,6)\n",
    "valtest =val(model,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_enginner\n",
    "import time \n",
    "start_time = time.time()\n",
    "alltic_list = [i for i in range(k)]\n",
    "\n",
    "def Feature_enginner(model, processed_df, full_list, alltic_list, batch_size=64, k=20, env_size=40,dela = 48):\n",
    "    back_iter = DataLoader(StockDataset_back([i for i in range(k)],full_list), shuffle=False, batch_size=batch_size, num_workers=0,drop_last=False)\n",
    "    \n",
    "    new_state = predict(model,back_iter)\n",
    "    print(1)\n",
    "    concatenated_tensor = torch.cat(new_state, dim=0)\n",
    "    print(2)\n",
    "    concatenated_tensor = concatenated_tensor.view(-1,env_size)\n",
    "    print(3)\n",
    "    feature_df = pd.DataFrame(concatenated_tensor.numpy())\n",
    "    print(4)\n",
    "    \n",
    "    full_delay_df = delaydate(dela ,processed_df)\n",
    "    \n",
    "    merged_df = pd.concat([full_delay_df, feature_df], axis=1)\n",
    "    merged_df = merged_df.loc[:,['tic','date','close'] + INDICATORS + [i for i in range(env_size)]]\n",
    "    merged_df.columns = ['tic','date','close'] + INDICATORS + [f\"temporal_feature_{i}\" for i in range(env_size)]\n",
    "    end_time = time.time()\n",
    "    print('总计消耗时间:',(end_time - start_time)/60)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addfeature_df = Feature_enginner(model, processed_df1, array_full_list, alltic_list, batch_size=64, k=50 ,env_size=20, dela=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tic</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>...</th>\n",
       "      <th>temporal_feature_10</th>\n",
       "      <th>temporal_feature_11</th>\n",
       "      <th>temporal_feature_12</th>\n",
       "      <th>temporal_feature_13</th>\n",
       "      <th>temporal_feature_14</th>\n",
       "      <th>temporal_feature_15</th>\n",
       "      <th>temporal_feature_16</th>\n",
       "      <th>temporal_feature_17</th>\n",
       "      <th>temporal_feature_18</th>\n",
       "      <th>temporal_feature_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600000.SH</td>\n",
       "      <td>2017-03-17</td>\n",
       "      <td>16.20</td>\n",
       "      <td>-0.103361</td>\n",
       "      <td>16.920131</td>\n",
       "      <td>16.024869</td>\n",
       "      <td>42.451553</td>\n",
       "      <td>-109.290506</td>\n",
       "      <td>24.313216</td>\n",
       "      <td>16.560333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500900</td>\n",
       "      <td>-0.444595</td>\n",
       "      <td>2.496382</td>\n",
       "      <td>1.498984</td>\n",
       "      <td>-1.503451</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>1.428747</td>\n",
       "      <td>0.559653</td>\n",
       "      <td>-1.066029</td>\n",
       "      <td>2.959662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>600000.SH</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>16.14</td>\n",
       "      <td>-0.110808</td>\n",
       "      <td>16.854846</td>\n",
       "      <td>16.013154</td>\n",
       "      <td>40.998417</td>\n",
       "      <td>-121.936983</td>\n",
       "      <td>25.144384</td>\n",
       "      <td>16.543000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.900961</td>\n",
       "      <td>-0.317013</td>\n",
       "      <td>2.691144</td>\n",
       "      <td>2.069022</td>\n",
       "      <td>-1.325521</td>\n",
       "      <td>0.301127</td>\n",
       "      <td>1.796261</td>\n",
       "      <td>-0.097198</td>\n",
       "      <td>-1.664815</td>\n",
       "      <td>3.305223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600000.SH</td>\n",
       "      <td>2017-03-21</td>\n",
       "      <td>16.00</td>\n",
       "      <td>-0.126338</td>\n",
       "      <td>16.798334</td>\n",
       "      <td>15.981666</td>\n",
       "      <td>37.869445</td>\n",
       "      <td>-145.491419</td>\n",
       "      <td>36.351888</td>\n",
       "      <td>16.520667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.183464</td>\n",
       "      <td>-0.027345</td>\n",
       "      <td>2.698100</td>\n",
       "      <td>2.387536</td>\n",
       "      <td>-1.106515</td>\n",
       "      <td>0.843225</td>\n",
       "      <td>1.773876</td>\n",
       "      <td>-0.257767</td>\n",
       "      <td>-1.632134</td>\n",
       "      <td>3.319397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600000.SH</td>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>15.78</td>\n",
       "      <td>-0.154311</td>\n",
       "      <td>16.797444</td>\n",
       "      <td>15.885556</td>\n",
       "      <td>33.689702</td>\n",
       "      <td>-193.198499</td>\n",
       "      <td>50.992976</td>\n",
       "      <td>16.491000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.220645</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>2.916779</td>\n",
       "      <td>2.445098</td>\n",
       "      <td>-1.038702</td>\n",
       "      <td>0.922410</td>\n",
       "      <td>1.836789</td>\n",
       "      <td>-0.038941</td>\n",
       "      <td>-1.385233</td>\n",
       "      <td>3.098996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>600000.SH</td>\n",
       "      <td>2017-03-23</td>\n",
       "      <td>15.88</td>\n",
       "      <td>-0.166606</td>\n",
       "      <td>16.770306</td>\n",
       "      <td>15.831694</td>\n",
       "      <td>36.961361</td>\n",
       "      <td>-156.439114</td>\n",
       "      <td>50.992976</td>\n",
       "      <td>16.463000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.768891</td>\n",
       "      <td>-0.994125</td>\n",
       "      <td>2.443021</td>\n",
       "      <td>2.249985</td>\n",
       "      <td>-0.970342</td>\n",
       "      <td>-1.083996</td>\n",
       "      <td>1.311004</td>\n",
       "      <td>-1.812655</td>\n",
       "      <td>-2.678623</td>\n",
       "      <td>4.134655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70545</th>\n",
       "      <td>601998.SH</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.031323</td>\n",
       "      <td>5.190485</td>\n",
       "      <td>4.833515</td>\n",
       "      <td>50.959756</td>\n",
       "      <td>-36.727817</td>\n",
       "      <td>0.176333</td>\n",
       "      <td>4.928000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.779195</td>\n",
       "      <td>-0.934460</td>\n",
       "      <td>1.517315</td>\n",
       "      <td>-1.088990</td>\n",
       "      <td>-0.973530</td>\n",
       "      <td>-1.664009</td>\n",
       "      <td>0.232380</td>\n",
       "      <td>2.426849</td>\n",
       "      <td>-0.475797</td>\n",
       "      <td>-0.119264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70546</th>\n",
       "      <td>601998.SH</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>5.174477</td>\n",
       "      <td>4.825523</td>\n",
       "      <td>52.808796</td>\n",
       "      <td>-19.119878</td>\n",
       "      <td>2.625621</td>\n",
       "      <td>4.935333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.734905</td>\n",
       "      <td>-0.892610</td>\n",
       "      <td>1.788705</td>\n",
       "      <td>-0.985208</td>\n",
       "      <td>-1.021302</td>\n",
       "      <td>-1.598934</td>\n",
       "      <td>0.435053</td>\n",
       "      <td>2.181200</td>\n",
       "      <td>-0.728077</td>\n",
       "      <td>0.176656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70547</th>\n",
       "      <td>601998.SH</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.026590</td>\n",
       "      <td>5.171599</td>\n",
       "      <td>4.823401</td>\n",
       "      <td>54.822659</td>\n",
       "      <td>8.610136</td>\n",
       "      <td>14.173296</td>\n",
       "      <td>4.946667</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.004894</td>\n",
       "      <td>-0.915082</td>\n",
       "      <td>1.391061</td>\n",
       "      <td>-1.209658</td>\n",
       "      <td>-0.966595</td>\n",
       "      <td>-1.683521</td>\n",
       "      <td>0.205795</td>\n",
       "      <td>2.563425</td>\n",
       "      <td>-0.428341</td>\n",
       "      <td>-0.234574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70548</th>\n",
       "      <td>601998.SH</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.023347</td>\n",
       "      <td>5.171576</td>\n",
       "      <td>4.818424</td>\n",
       "      <td>53.350646</td>\n",
       "      <td>-44.954584</td>\n",
       "      <td>9.544348</td>\n",
       "      <td>4.957333</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.267627</td>\n",
       "      <td>-0.967534</td>\n",
       "      <td>0.878120</td>\n",
       "      <td>-1.595519</td>\n",
       "      <td>-0.918358</td>\n",
       "      <td>-1.892846</td>\n",
       "      <td>-0.190184</td>\n",
       "      <td>2.897044</td>\n",
       "      <td>-0.247745</td>\n",
       "      <td>-0.814026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70549</th>\n",
       "      <td>601998.SH</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.024528</td>\n",
       "      <td>5.171217</td>\n",
       "      <td>4.824783</td>\n",
       "      <td>54.611375</td>\n",
       "      <td>4.080156</td>\n",
       "      <td>16.759509</td>\n",
       "      <td>4.971000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.083955</td>\n",
       "      <td>-1.051309</td>\n",
       "      <td>1.262324</td>\n",
       "      <td>-1.443753</td>\n",
       "      <td>-1.001035</td>\n",
       "      <td>-1.967390</td>\n",
       "      <td>-0.204718</td>\n",
       "      <td>2.577083</td>\n",
       "      <td>-0.514137</td>\n",
       "      <td>-0.255775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70550 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tic        date  close      macd    boll_ub    boll_lb  \\\n",
       "0      600000.SH  2017-03-17  16.20 -0.103361  16.920131  16.024869   \n",
       "1      600000.SH  2017-03-20  16.14 -0.110808  16.854846  16.013154   \n",
       "2      600000.SH  2017-03-21  16.00 -0.126338  16.798334  15.981666   \n",
       "3      600000.SH  2017-03-22  15.78 -0.154311  16.797444  15.885556   \n",
       "4      600000.SH  2017-03-23  15.88 -0.166606  16.770306  15.831694   \n",
       "...          ...         ...    ...       ...        ...        ...   \n",
       "70545  601998.SH  2022-12-26   4.83  0.031323   5.190485   4.833515   \n",
       "70546  601998.SH  2022-12-27   4.90  0.025253   5.174477   4.825523   \n",
       "70547  601998.SH  2022-12-28   4.98  0.026590   5.171599   4.823401   \n",
       "70548  601998.SH  2022-12-29   4.93  0.023347   5.171576   4.818424   \n",
       "70549  601998.SH  2022-12-30   4.98  0.024528   5.171217   4.824783   \n",
       "\n",
       "          rsi_30      cci_30      dx_30  close_30_sma  ...  \\\n",
       "0      42.451553 -109.290506  24.313216     16.560333  ...   \n",
       "1      40.998417 -121.936983  25.144384     16.543000  ...   \n",
       "2      37.869445 -145.491419  36.351888     16.520667  ...   \n",
       "3      33.689702 -193.198499  50.992976     16.491000  ...   \n",
       "4      36.961361 -156.439114  50.992976     16.463000  ...   \n",
       "...          ...         ...        ...           ...  ...   \n",
       "70545  50.959756  -36.727817   0.176333      4.928000  ...   \n",
       "70546  52.808796  -19.119878   2.625621      4.935333  ...   \n",
       "70547  54.822659    8.610136  14.173296      4.946667  ...   \n",
       "70548  53.350646  -44.954584   9.544348      4.957333  ...   \n",
       "70549  54.611375    4.080156  16.759509      4.971000  ...   \n",
       "\n",
       "       temporal_feature_10  temporal_feature_11  temporal_feature_12  \\\n",
       "0                 1.500900            -0.444595             2.496382   \n",
       "1                 1.900961            -0.317013             2.691144   \n",
       "2                 2.183464            -0.027345             2.698100   \n",
       "3                 2.220645             0.032896             2.916779   \n",
       "4                 1.768891            -0.994125             2.443021   \n",
       "...                    ...                  ...                  ...   \n",
       "70545            -0.779195            -0.934460             1.517315   \n",
       "70546            -0.734905            -0.892610             1.788705   \n",
       "70547            -1.004894            -0.915082             1.391061   \n",
       "70548            -1.267627            -0.967534             0.878120   \n",
       "70549            -1.083955            -1.051309             1.262324   \n",
       "\n",
       "       temporal_feature_13  temporal_feature_14  temporal_feature_15  \\\n",
       "0                 1.498984            -1.503451            -0.268903   \n",
       "1                 2.069022            -1.325521             0.301127   \n",
       "2                 2.387536            -1.106515             0.843225   \n",
       "3                 2.445098            -1.038702             0.922410   \n",
       "4                 2.249985            -0.970342            -1.083996   \n",
       "...                    ...                  ...                  ...   \n",
       "70545            -1.088990            -0.973530            -1.664009   \n",
       "70546            -0.985208            -1.021302            -1.598934   \n",
       "70547            -1.209658            -0.966595            -1.683521   \n",
       "70548            -1.595519            -0.918358            -1.892846   \n",
       "70549            -1.443753            -1.001035            -1.967390   \n",
       "\n",
       "       temporal_feature_16  temporal_feature_17  temporal_feature_18  \\\n",
       "0                 1.428747             0.559653            -1.066029   \n",
       "1                 1.796261            -0.097198            -1.664815   \n",
       "2                 1.773876            -0.257767            -1.632134   \n",
       "3                 1.836789            -0.038941            -1.385233   \n",
       "4                 1.311004            -1.812655            -2.678623   \n",
       "...                    ...                  ...                  ...   \n",
       "70545             0.232380             2.426849            -0.475797   \n",
       "70546             0.435053             2.181200            -0.728077   \n",
       "70547             0.205795             2.563425            -0.428341   \n",
       "70548            -0.190184             2.897044            -0.247745   \n",
       "70549            -0.204718             2.577083            -0.514137   \n",
       "\n",
       "       temporal_feature_19  \n",
       "0                 2.959662  \n",
       "1                 3.305223  \n",
       "2                 3.319397  \n",
       "3                 3.098996  \n",
       "4                 4.134655  \n",
       "...                    ...  \n",
       "70545            -0.119264  \n",
       "70546             0.176656  \n",
       "70547            -0.234574  \n",
       "70548            -0.814026  \n",
       "70549            -0.255775  \n",
       "\n",
       "[70550 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addfeature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addfeature_df.to_csv('1227_addfeature_df.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "addfeature_df = pd.read_csv('1227_addfeature_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('1228_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['date'] = pd.to_datetime(merged_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addfeature_df['date'] = pd.to_datetime(addfeature_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df1 = merged_df[['trade_date','ts_code','filled','ema']]\n",
    "merged_df1['date'] = pd.to_datetime(merged_df1['trade_date'])\n",
    "merged_df1['tic'] = merged_df1['ts_code']\n",
    "merged_df1 = merged_df1.loc[:,('date','tic','filled','ema')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "addfeature_df1 = pd.merge(addfeature_df,merged_df1,on=('date','tic'),how='left')\n",
    "addfeature_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df3.to_csv('1227_processed.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df3 = pd.read_csv('1227_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df3['date'] = pd.to_datetime(processed_df3['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addfeature_df[addfeature_df['tic'] == '601939.SH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triu(g,diagonal=1+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triu(g,diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Build A Market Environment in OpenAI Gym-style\n",
    "The training process involves observing stock price change, taking an action and reward's calculation. By interacting with the market environment, the agent will eventually derive a trading strategy that may maximize (expected) rewards.\n",
    "\n",
    "Our market environment, based on OpenAI Gym, simulates stock markets with historical market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TOhcryx44bb"
   },
   "source": [
    "## Data Split\n",
    "We split the data into training set and testing set as follows:\n",
    "\n",
    "Training data period: 2009-01-01 to 2020-07-01\n",
    "\n",
    "Trading data period: 2020-07-01 to 2021-10-31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2015-03-02'\n",
    "TRAIN_END_DATE = '2020-07-30'\n",
    "TRADE_START_DATE = '2020-07-31'\n",
    "TRADE_END_DATE = '2021-02-26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = addfeature_df['date'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0qaVGjLtgbI",
    "outputId": "ac9f2699-65c6-418f-cca4-f0f166973f65"
   },
   "outputs": [],
   "source": [
    "train = data_split(addfeature_df1, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(addfeature_df1, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addfeature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "p52zNCOhTtLR",
    "outputId": "14568787-e92d-4d9b-dbce-9e11b3fb1390"
   },
   "outputs": [],
   "source": [
    "train.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfattn_indicator_list = [f\"temporal_feature_{i}\" for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfattn_indicator_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "k9zU9YaTTvFq",
    "outputId": "80e64947-4c5c-428f-a98c-6b344d816783"
   },
   "outputs": [],
   "source": [
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYN573SOHhxG",
    "outputId": "460cd177-65d2-46f8-eb40-eaeda045c4bb"
   },
   "outputs": [],
   "source": [
    "INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_dimension = len(train.tic.unique())\n",
    "# state_space = 1 + 2*stock_dimension + len(selfattn_indicator_list)*stock_dimension\n",
    "# print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "d2e2b678-04f8-44be-96ea-3523903780eb"
   },
   "outputs": [],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 4*stock_dimension + len(INDICATORS)*stock_dimension + len(selfattn_indicator_list)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 500,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"risk_preference\": 'ne',   #ne为中性、pr为激进,除此之外都是保守策略\n",
    "    'adjust': True,  #True为启用动态调整机制，False关闭\n",
    "    \"reward_scaling\": 1,\n",
    "    \"attn_indicator_list\":selfattn_indicator_list,\n",
    "    \"alpha\": 100, #风险喜好函数系数\n",
    "    \"beta\" : 100, #风险中性函数系数\n",
    "    \"c\" : 100, # 风险厌恶函数系数\n",
    "    \"theta\" : 0.2, # 风险喜好放缩因子\n",
    "    \"gamma\" : 5, # 风险厌恶放缩因子\n",
    "    \"mu\" : 1, #惩罚项放缩因子\n",
    "    \"p_lambda\" : 100,  #惩罚系数\n",
    "    'up_ad_thre': 0.15, #动态调整机制中向上变化率阈值\n",
    "    'down_ad_thre': 0.1,  #动态调整机制中向下变化率阈值\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64EoqOrQjiVf"
   },
   "source": [
    "## Environment for Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwSvvPjutpqS",
    "outputId": "3645668e-ba1f-4610-9789-bc212eb9b776"
   },
   "outputs": [],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Train DRL Agents\n",
    "* The DRL algorithms are from **Stable Baselines 3**. Users are also encouraged to try **ElegantRL** and **Ray RLlib**.\n",
    "* FinRL includes fine-tuned standard DRL algorithms, such as DQN, DDPG, Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = False\n",
    "if_using_td3 = True\n",
    "if_using_sac = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "### Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "a2bdd15c-c366-4f89-e6de-6b0572c2ad23"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GVpkWGqH4-D"
   },
   "outputs": [],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2YadjfnLwgt",
    "outputId": "211f6ae4-c91e-41df-8906-df246e090f75"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5D5PFUhMzSV",
    "outputId": "3405d353-0a08-4855-ca59-98098968dd11"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt8eIQKYM4G3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=30000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSAHhV4Xc-bh",
    "outputId": "802fef04-8df5-4df2-f710-e5b08941842f"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=30000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwOhVjqRkCdM",
    "outputId": "3c848ff6-587e-43d1-b50d-a7d521a14b4f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8RSdKCckJyH",
    "outputId": "4d65a11b-32b0-4c3f-b0da-e1b59fc6e194",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=30000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2wZgkQXh1jE"
   },
   "source": [
    "## In-sample Performance\n",
    "\n",
    "Assume that the initial capital is $1,000,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEv5KGC8h1jE"
   },
   "source": [
    "### Set turbulence threshold\n",
    "Set the turbulence threshold to be greater than the maximum of insample turbulence data. If current turbulence index is greater than the threshold, then we assume that the current market is volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efwBi84ch1jE"
   },
   "outputs": [],
   "source": [
    "data_risk_indicator = addfeature_df[(addfeature_df.date<TRAIN_END_DATE) & (addfeature_df.date>=TRAIN_START_DATE)]\n",
    "insample_risk_indicator = data_risk_indicator.drop_duplicates(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHZMBpSqh1jG",
    "outputId": "73edde3b-1414-4a0d-f876-6460de2ff8e1"
   },
   "outputs": [],
   "source": [
    "insample_risk_indicator.vix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDkszkMloRWT",
    "outputId": "c5df1f2f-ed52-4107-a3af-a358767ae155"
   },
   "outputs": [],
   "source": [
    "insample_risk_indicator.vix.quantile(0.996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AL7hs7svnNWT",
    "outputId": "ab5a1015-3f83-41b7-ba65-1487f3188588"
   },
   "outputs": [],
   "source": [
    "insample_risk_indicator.turbulence.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N78hfHckoqJ9",
    "outputId": "2c8a54d6-022f-4716-dc6d-22a450f5b9ab"
   },
   "outputs": [],
   "source": [
    "insample_risk_indicator.turbulence.quantile(0.996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5mmgQF_h1jQ"
   },
   "source": [
    "### Trading (Out-of-sample Performance)\n",
    "\n",
    "We update periodically in order to take full advantage of the data, e.g., retrain quarterly, monthly or weekly. We also tune the parameters along the way, in this notebook we use the in-sample data from 2009-01 to 2020-07 to tune the parameters once, so there is some alpha decay here as the length of trade date extends. \n",
    "\n",
    "Numerous hyperparameters – e.g. the learning rate, the total number of samples to train on – influence the learning process and are usually determined by testing some variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIqoV0GSI52v"
   },
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade,**env_kwargs)\n",
    "# env_trade, obs_trade = e_trade_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "W_XNgGsBMeVw",
    "outputId": "56d2af8e-7ee7-4248-ce7a-3132625c61b1"
   },
   "outputs": [],
   "source": [
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLOnL5eYh1jR",
    "outputId": "d31d2209-05ef-41df-fd8f-ee081d427949"
   },
   "outputs": [],
   "source": [
    "trained_moedl = trained_td3\n",
    "df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
    "    model=trained_moedl, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value.to_csv('107_up_up_df_account_value.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERxw3KqLkcP4",
    "outputId": "780c7269-0139-42e0-8ad6-081fd93717bf"
   },
   "outputs": [],
   "source": [
    "df_account_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2yRkNguY5yvp",
    "outputId": "13f1ace3-c343-4bee-e0c1-214b2f8a1372"
   },
   "outputs": [],
   "source": [
    "df_account_value.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "nFlK5hNbWVFk",
    "outputId": "a635843e-7732-47ce-8044-9c87a33de736"
   },
   "outputs": [],
   "source": [
    "df_actions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtesting Results\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "216032a2-5566-4dba-db83-7d7d880b1e88"
   },
   "outputs": [],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "#now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "#perf_stats_all.to_csv(\"./\"+RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "#now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "#perf_stats_all.to_csv(\"./\"+RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "#now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "#perf_stats_all.to_csv(\"./\"+RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tic      date    weight\n",
      "0   600000.SH  20160229  5.216948\n",
      "1   600010.SH  20160229  0.813992\n",
      "2   600015.SH  20160229  1.507985\n",
      "3   600016.SH  20160229  7.570924\n",
      "4   600018.SH  20160229  0.455995\n",
      "5   600028.SH  20160229  1.385986\n",
      "6   600030.SH  20160229  3.407966\n",
      "7   600036.SH  20160229  4.552954\n",
      "8   600048.SH  20160229  1.776982\n",
      "9   600050.SH  20160229  1.032990\n",
      "10  600104.SH  20160229  1.837982\n",
      "11  600109.SH  20160229  0.625994\n",
      "12  600111.SH  20160229  0.735993\n",
      "13  600150.SH  20160229  0.437996\n",
      "14  600518.SH  20160229  1.217988\n",
      "15  600519.SH  20160229  3.273967\n",
      "16  600585.SH  20160229  0.853991\n",
      "17  600637.SH  20160229  1.371986\n",
      "18  600795.SH  20160229  0.863991\n",
      "19  600837.SH  20160229  3.041970\n",
      "20  600887.SH  20160229  2.388976\n",
      "21  600893.SH  20160229  0.827992\n",
      "22  600958.SH  20160229  0.552994\n",
      "23  600999.SH  20160229  1.229988\n",
      "24  601006.SH  20160229  1.169988\n",
      "25  601088.SH  20160229  0.817992\n",
      "26  601166.SH  20160229  5.733943\n",
      "27  601169.SH  20160229  2.953970\n",
      "28  601186.SH  20160229  1.017990\n",
      "29  601211.SH  20160229  0.755992\n",
      "30  601288.SH  20160229  3.464965\n",
      "31  601318.SH  20160229  9.527905\n",
      "32  601328.SH  20160229  3.691963\n",
      "33  601336.SH  20160229  0.641994\n",
      "34  601390.SH  20160229  1.235988\n",
      "35  601398.SH  20160229  2.634974\n",
      "36  601601.SH  20160229  2.240978\n",
      "37  601628.SH  20160229  1.003990\n",
      "38  601668.SH  20160229  2.332977\n",
      "39  601669.SH  20160229  0.736993\n",
      "40  601688.SH  20160229  1.349987\n",
      "41  601766.SH  20160229  2.592974\n",
      "42  601800.SH  20160229  0.488995\n",
      "43  601818.SH  20160229  1.641984\n",
      "44  601857.SH  20160229  1.071989\n",
      "45  601901.SH  20160229  0.794992\n",
      "46  601985.SH  20160229  0.962990\n",
      "47  601988.SH  20160229  2.025980\n",
      "48  601989.SH  20160229  1.622984\n",
      "49  601998.SH  20160229  0.499995\n"
     ]
    }
   ],
   "source": [
    "#获取随机选择的tic在select_date的权重并修正\n",
    "def get_selected_index_weight(df_index,select_date,selected_tics):\n",
    "    df = df_index[df_index['trade_date'] == select_date]\n",
    "    select_df = df[df['con_code'].isin(selected_tics)]\n",
    "    select_df = select_df.drop('index_code',axis=1).rename(columns={'con_code':'tic','trade_date':'date'})\n",
    "    select_df = select_df.reset_index()\n",
    "    weight_sum = select_df['weight'].sum()\n",
    "#     print(select_df)\n",
    "    for i in range(len(select_df)):\n",
    "        select_df.loc[i,'weight'] = (select_df.loc[i,'weight'] / weight_sum)*100\n",
    "    return select_df.loc[:,['tic','date','weight']]\n",
    "\n",
    "baseline_weight =get_selected_index_weight(df_index,select_date,selected_tics)\n",
    "baseline_weight = baseline_weight.sort_values(['tic'],ascending=True).reset_index(drop=True)\n",
    "print(baseline_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_account_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m baseline\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# full_date_range = get_trading_days(exchange='SSE',start_date='20210301', end_date='20230227')\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# full_date_range = full_date_range.sort_values('trade_date',ascending=True).reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# subset_df = addfeature_df.loc[(addfeature_df['date'].astype(str) >='20210301') & (addfeature_df['date'].astype(str)<='20220227')]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m full_date_range1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mdf_account_value\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()})\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m baseline_sse \u001b[38;5;241m=\u001b[39m calculate_selected_baseline(addfeature_df,full_date_range1,selected_tics)\n\u001b[1;32m     20\u001b[0m baseline_sse\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_account_value' is not defined"
     ]
    }
   ],
   "source": [
    "#计算所选股票的buy & hold策略\n",
    "def calculate_selected_baseline(df,full_date_range,selected_tics):\n",
    "    df = df.sort_values(['tic','date'],ascending=True).reset_index(drop=True)\n",
    "    baseline = pd.DataFrame({'date':full_date_range['date']})\n",
    "    for i in range(len(full_date_range)):\n",
    "        temp_date = full_date_range.loc[i,'date']\n",
    "        temp_df = df[df['date'] == temp_date].sort_values('tic',ascending=True)\n",
    "        close = list(temp_df['close'])\n",
    "        weight = list(baseline_weight['weight'])\n",
    "        baseline.loc[i,'account_value'] = sum(np.array(close) * np.array(weight)) #注意权重和收盘价对应的股票顺序\n",
    "    baseline['date'] = pd.to_datetime(baseline['date'],format='%Y%m%d')\n",
    "#     baseline.set_index(\"date\", inplace=True, drop=True)\n",
    "#     baseline.index = baseline.index.tz_localize(\"UTC\")\n",
    "    return baseline\n",
    "# full_date_range = get_trading_days(exchange='SSE',start_date='20210301', end_date='20230227')\n",
    "# full_date_range = full_date_range.sort_values('trade_date',ascending=True).reset_index(drop=True)\n",
    "# subset_df = addfeature_df.loc[(addfeature_df['date'].astype(str) >='20210301') & (addfeature_df['date'].astype(str)<='20220227')]\n",
    "full_date_range1 = pd.DataFrame({'date':df_account_value['date'].unique()}).reset_index(drop=True)\n",
    "baseline_sse = calculate_selected_baseline(addfeature_df,full_date_range1,selected_tics)\n",
    "baseline_sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addfeature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sse.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_sse = pro.index_daily(ts_code='000016.sh',start_date = '20200301',end_date='20230227')\n",
    "# baseline_sse = baseline_sse.rename(columns={'trade_date':'date'})\n",
    "# baseline_sse = baseline_sse.sort_values('date',ascending=True)\n",
    "# print(baseline_sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkV-LB66iwhD",
    "outputId": "7231e720-1598-45ac-ff49-9f8d29f296fa"
   },
   "outputs": [],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "# baseline_df = get_baseline(\n",
    "#         ticker=\"^DJI\", \n",
    "#         start = df_account_value.loc[0,'date'],\n",
    "#         end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(baseline_sse, value_col_name = 'account_value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfolio\n",
    "from copy import deepcopy\n",
    "def backtest_plot_com(\n",
    "    account_value,\n",
    "    baseline,\n",
    "    baseline_start=TRADE_START_DATE,\n",
    "    baseline_end=TRADE_END_DATE,\n",
    "    value_col_name=\"account_value\",\n",
    "):\n",
    "    df = deepcopy(account_value)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    test_returns = get_daily_return(df, value_col_name=value_col_name)\n",
    "#     pro = ts.pro_api()\n",
    "#     baseline_df = pro.index_daily(ts_code='000016.sh',start_date = '20200301',end_date='20230227')\n",
    "#     baseline_df = baseline_df.rename(columns={'trade_date':'date'})\n",
    "#     baseline_df = baseline_df.sort_values('date',ascending=True)\n",
    "    baseline_returns = get_daily_return(baseline, value_col_name=value_col_name)\n",
    "    with pyfolio.plotting.plotting_context(font_scale=1.1):\n",
    "        pyfolio.create_full_tear_sheet(\n",
    "            returns=test_returns, benchmark_rets=baseline_returns, set_context=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"==============Compare to SSE50===========\")\n",
    "# from finrl.plot import backtest_plot_com\n",
    "%matplotlib inline\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "backtest_plot_com(account_value = df_account_value, \n",
    "                  baseline = baseline_sse,\n",
    "                  baseline_start = df_account_value.loc[len(df_account_value)-1,'date'],\n",
    "                  baseline_end = df_account_value.loc[0,'date']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lKRGftSS7pNM",
    "outputId": "db260938-69a7-4417-fed7-25679d43b0f6"
   },
   "outputs": [],
   "source": [
    "print(\"==============Compare to SSE50===========\")\n",
    "# from finrl.plot import backtest_plot_com\n",
    "%matplotlib inline\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "backtest_plot_com(account_value = df_account_value, \n",
    "                  baseline = baseline_sse,\n",
    "                  baseline_start = df_account_value.loc[len(df_account_value)-1,'date'],\n",
    "                  baseline_end = df_account_value.loc[0,'date']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==============Compare to SSE50===========\")\n",
    "# from finrl.plot import backtest_plot_com\n",
    "%matplotlib inline\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "backtest_plot_com(account_value = df_account_value, \n",
    "                  baseline = baseline_sse,\n",
    "                  baseline_start = df_account_value.loc[len(df_account_value)-1,'date'],\n",
    "                  baseline_end = df_account_value.loc[0,'date']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_account_value.loc[len(df_account_value)-1,'date'],df_account_value.loc[0,'date'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv"
   ],
   "name": "Stock_NeurIPS2018.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
